{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import sys\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "#from bs4 import BeautifulSoup"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1129)>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model_file = (os.path.join(sys.path[0],'GoogleNews-vectors-negative300.bin'))\n",
    "#wordmodel = Word2Vec(model_file)\n",
    "wordmodel_new= gensim.models.KeyedVectors.load_word2vec_format(model_file, binary=True,limit =1000000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "source": [
    "cleaned_data = pd.read_csv(os.path.join(sys.path[0],'cleaned_data.csv')) # load cleaned reviews \n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "source": [
    "cleaned_data=cleaned_data.dropna() # Dropping reviews that have type NaN as this will cause when randomly selecting classes with a particular class\n",
    "cleaned_data = cleaned_data.reset_index(drop=True) # reseting the index tso that it covers up for the dropped values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "cleaned_data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>bin_class</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>little awkward use great concept depending ski...</td>\n",
       "      <td>3</td>\n",
       "      <td>['A', 'little', 'awkward', 'to', 'use', '.', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "      <td>3</td>\n",
       "      <td>brita hard sided water filter bottle decent wa...</td>\n",
       "      <td>3</td>\n",
       "      <td>['The', 'Brita', 'Hard', 'Sided', 'Water', 'Fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>blender look like new centering pad</td>\n",
       "      <td>1</td>\n",
       "      <td>['Our', 'blender', 'looks', 'like', 'new', 'wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>bought boyfriend loved</td>\n",
       "      <td>1</td>\n",
       "      <td>['Bought', 'it', 'for', 'my', 'boyfriend', 'he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>124</td>\n",
       "      <td>4</td>\n",
       "      <td>like dish color clear redcranberry color swirl...</td>\n",
       "      <td>1</td>\n",
       "      <td>['What', 'I', 'like', 'about', 'the', 'dishes'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  star_rating                                        review_body  \\\n",
       "0          75            3  little awkward use great concept depending ski...   \n",
       "1          94            3  brita hard sided water filter bottle decent wa...   \n",
       "2         100            4                blender look like new centering pad   \n",
       "3         103            4                             bought boyfriend loved   \n",
       "4         124            4  like dish color clear redcranberry color swirl...   \n",
       "\n",
       "   bin_class                                             tokens  \n",
       "0          3  ['A', 'little', 'awkward', 'to', 'use', '.', '...  \n",
       "1          3  ['The', 'Brita', 'Hard', 'Sided', 'Water', 'Fi...  \n",
       "2          1  ['Our', 'blender', 'looks', 'like', 'new', 'wi...  \n",
       "3          1  ['Bought', 'it', 'for', 'my', 'boyfriend', 'he...  \n",
       "4          1  ['What', 'I', 'like', 'about', 'the', 'dishes'...  "
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "source": [
    "cleaned_data = cleaned_data.loc[cleaned_data[\"bin_class\"] != 3] # remove neutral reviews"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "source": [
    "# I am selecting a smaller dataset due to hardware limitations\n",
    "cleaned_data['bin_class']=cleaned_data['bin_class'].astype(int)\n",
    "s1 = cleaned_data.bin_class[cleaned_data.bin_class.eq(0)].sample(40000,random_state=1).index\n",
    "s2 = cleaned_data.bin_class[cleaned_data.bin_class.eq(1)].sample(40000,random_state=1).index \n",
    "\n",
    "\n",
    "\n",
    "cleaned_data = cleaned_data.loc[s1.union(s2)]\n",
    "display(cleaned_data['bin_class'].value_counts())\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "1    40000\n",
       "0    40000\n",
       "Name: bin_class, dtype: int64"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "cleaned_data['tokens'] = cleaned_data['review_body'].apply(word_tokenize)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "# finding the average of each review vectors\n",
    "def averageVector_wordmodel(tokens, w2v):\n",
    "    all_avg = []\n",
    "    x = w2v.index_to_key\n",
    "    for i in tokens:\n",
    "        count = 0\n",
    "        flag = True\n",
    "        vec_avg = [0]*300\n",
    "        \n",
    "        \n",
    "        for word in i:\n",
    "            if word in x:\n",
    "                count += 1\n",
    "                vec_avg = np.add(vec_avg, w2v[word])\n",
    "                flag = False\n",
    "        if not flag:\n",
    "            vec_avg = np.divide(vec_avg, count)\n",
    "        all_avg.append(vec_avg)\n",
    "    return all_avg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "allVectors_wordmodel = averageVector_wordmodel(cleaned_data['tokens'], wordmodel_new) #calling the method to find avg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "avg_df_word = pd.DataFrame(allVectors_wordmodel) # creating a data frame to reduce time "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "avg_df_word.to_csv(os.path.join(sys.path[0],'avg_vector_word.csv',index=False)) #saving avg vectors for reproducibilty "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "ret_avg_df_word = pd.read_csv(os.path.join(sys.path[0],'avg_vector_word.csv')) # loading it"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y2_word = cleaned_data[\"bin_class\"]\n",
    "X_train2_word, X_test2_word, y_train2_word, y_test2_word = train_test_split(ret_avg_df_word, y2_word, test_size=0.2, random_state=42, stratify = y2_word)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "p_word = Perceptron(random_state=42)\n",
    "p_word.fit(X_train2_word, y_train2_word)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Perceptron(random_state=42)"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "print('Perceptron Model Pre Trained')\n",
    "print('Accuracy of Test: ', accuracy_score(y_test2_word,p_word.predict(X_test2_word) ) * 100,'%')\n",
    "print('Precision of Test: ',precision_score(y_test2_word, p_word.predict(X_test2_word), average='macro') *100,'%')\n",
    "print('Recall of Test: ', recall_score(y_test2_word,p_word.predict(X_test2_word) ) * 100,'%')\n",
    "print('F1 Score of Test: ', f1_score(y_test2_word,p_word.predict(X_test2_word) ) * 100,'%')\n",
    "\n",
    "print('Accuracy of Train: ', accuracy_score(y_train2_word,p_word.predict(X_train2_word) ) * 100,'%')\n",
    "print('Precision of Train: ',precision_score(y_train2_word, p_word.predict(X_train2_word), average='macro') *100,'%')\n",
    "print('Recall of Train: ', recall_score(y_train2_word,p_word.predict(X_train2_word) ) * 100,'%')\n",
    "print('F1 Score of Train: ', f1_score(y_train2_word,p_word.predict(X_train2_word) ) * 100,'%')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Perceptron Model Pre Trained\n",
      "Accuracy of Test:  72.94375 %\n",
      "Precision of Test:  78.57343061298279 %\n",
      "Recall of Test:  95.1375 %\n",
      "F1 Score of Test:  77.85791008132577 %\n",
      "Accuracy of Train:  72.6140625 %\n",
      "Precision of Train:  78.16382115703564 %\n",
      "Recall of Train:  94.80937499999999 %\n",
      "F1 Score of Train:  77.58838948916309 %\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm_model_word = LinearSVC(random_state=8)\n",
    "svm_model_word.fit(X_train2_word, y_train2_word)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LinearSVC(random_state=8)"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "source": [
    "print('SVM Model Pre Trained')\n",
    "\n",
    "print('Accuracy of Test: ', accuracy_score(y_test2_word,svm_model_word.predict(X_test2_word) ) * 100,'%')\n",
    "print('Precision of Test: ',precision_score(y_test2_word, svm_model_word.predict(X_test2_word), average='macro') *100,'%')\n",
    "print('Recall of Test: ', recall_score(y_test2_word,svm_model_word.predict(X_test2_word) ) * 100,'%')\n",
    "print('F1 Score of Test: ', f1_score(y_test2_word,svm_model_word.predict(X_test2_word) ) * 100,'%')\n",
    "\n",
    "print('Accuracy of Train: ', accuracy_score(y_train2_word,svm_model_word.predict(X_train2_word) ) * 100,'%')\n",
    "print('Precision of Train: ',precision_score(y_train2_word, svm_model_word.predict(X_train2_word), average='macro') *100,'%')\n",
    "print('Recall of Train: ', recall_score(y_train2_word,svm_model_word.predict(X_train2_word) ) * 100,'%')\n",
    "print('F1 Score of Train: ', f1_score(y_train2_word,svm_model_word.predict(X_train2_word) ) * 100,'%')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SVM Model Pre Trained\n",
      "Accuracy of Test:  82.15 %\n",
      "Precision of Test:  82.19649173406398 %\n",
      "Recall of Test:  80.25 %\n",
      "F1 Score of Test:  81.80428134556574 %\n",
      "Accuracy of Train:  81.9921875 %\n",
      "Precision of Train:  82.04597469695031 %\n",
      "Recall of Train:  79.94375000000001 %\n",
      "F1 Score of Train:  81.61559444240616 %\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pip install torch"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\shubh\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING: You are using pip version 20.0.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\shubh\\Anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "# Class defintion of Feedforward Neural Network with 2 hidden layers\n",
    "class FF_NL(nn.Module):\n",
    "    def __init__(self, input_val, hidden_v1, hidden_v2, output_val):\n",
    "        \n",
    "        super(FF_NL, self).__init__()\n",
    "        self.fcl1 = nn.Linear(input_val, hidden_v1) \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fcl2 = nn.Linear(hidden_v1, hidden_v2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fcl3 = nn.Linear(hidden_v2, output_val)  \n",
    "\n",
    "    def forward(self, x_val):\n",
    "        out_val = self.fcl1(x_val)\n",
    "        out_val = self.relu1(out_val)\n",
    "        out_val = self.fcl2(out_val)\n",
    "        out_val = self.relu2(out_val)\n",
    "        out_val = self.fcl3(out_val)\n",
    "\n",
    "        return torch.sigmoid(out_val) # sigmoid for binary classification"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "source": [
    "# retriving avg vector for my model\n",
    "ret_avg_df_nn_word = pd.read_csv(os.path.join(sys.path[0],'avg_vector_word.csv'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "source": [
    "# split for binary classification using Word2Vec vectors on pre trained model\n",
    "y2_nn = cleaned_data[\"bin_class\"]\n",
    "X_train2_nn, X_test2_nn, y_train2_nn, y_test2_nn = train_test_split(ret_avg_df_nn_word, y2_nn, test_size=0.2, random_state=18, stratify = y2_nn)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "source": [
    "# initialising variables\n",
    "input_dim = 300\n",
    "hidden_dim1 = 50\n",
    "hidden_dim2 =10\n",
    "output_dim = 2\n",
    "num_epochs = 10\n",
    "\n",
    "ff_nn_w2v_my_model = FF_NL(input_dim, hidden_dim1,hidden_dim2, output_dim)\n",
    "loss_fn_est = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ff_nn_w2v_my_model.parameters(), lr=0.001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "source": [
    "ff_nn_w2v_my_model.train()\n",
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    loss_train = 0\n",
    "    for idx in range(len(X_train2_nn)):\n",
    "        \n",
    "        x_inp = X_train2_nn.iloc[idx]\n",
    "        x_inp = torch.tensor(x_inp)\n",
    "        x_inp = torch.reshape(x_inp,(1,300)).float()\n",
    "       \n",
    "        x_prob = ff_nn_w2v_my_model(x_inp)\n",
    "        x_prob = torch.reshape(x_prob,(1,2))\n",
    "        \n",
    "        y_inp = torch.tensor([y_train2_nn.iloc[idx]])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn_est(x_prob, y_inp)\n",
    "        \n",
    "        loss_train += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        tr+=1\n",
    "        temp+=1\n",
    "\n",
    "    \n",
    "    print('Epoch:', (epoch + 1), 'loss =', '{:.6f}'.format((loss_train / len(X_train2_nn))))\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1 loss = 0.502068\n",
      "Epoch: 2 loss = 0.484728\n",
      "Epoch: 3 loss = 0.478132\n",
      "Epoch: 4 loss = 0.473435\n",
      "Epoch: 5 loss = 0.469957\n",
      "Epoch: 6 loss = 0.466483\n",
      "Epoch: 7 loss = 0.464472\n",
      "Epoch: 8 loss = 0.462123\n",
      "Epoch: 9 loss = 0.461637\n",
      "Epoch: 10 loss = 0.460838\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# finding report on test split\n",
    "from sklearn.metrics import classification_report\n",
    "test_pred = []\n",
    "orig_label = []\n",
    "tp = 0\n",
    "sa = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(X_test2_nn)):\n",
    "        \n",
    "        cxv = X_test2_nn.iloc[idx]\n",
    "        cxv = torch.tensor(cxv)\n",
    "        cxv = torch.reshape(cxv,(1,300)).float()\n",
    "        probs_t = ff_nn_w2v_my_model(cxv)\n",
    "        vcx = torch.tensor([y_test2_nn.iloc[idx]])\n",
    "        probs_t = torch.reshape(probs_t,(1,2))\n",
    "        test_pred.append(torch.argmax(probs_t, dim=1).numpy()[0])\n",
    "        orig_label.append(vcx.numpy()[0])\n",
    "print(classification_report(orig_label,test_pred))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "source": [
    "# method to concatenate vectors for first 10 review words\n",
    "def concat_w2v(tokens, my_model):\n",
    "    all_avg = []\n",
    "    x = my_model.index_to_key\n",
    "    for i in tokens:\n",
    "        count = 0\n",
    "        flag = True\n",
    "        vec_avg = [0]*300\n",
    "        max_10 = 0\n",
    "        for word in i:\n",
    "            if max_10 ==10:\n",
    "                break\n",
    "            if word in x:\n",
    "                max_10+=1\n",
    "                count += 1\n",
    "                vec_avg = np.add(vec_avg, my_model[word])\n",
    "        all_avg.append(vec_avg)\n",
    "    return all_avg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "source": [
    "all_vec10 = concat_w2v(cleaned_data['tokens'], wordmodel_new) # calling the method"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "source": [
    "avg_df_10 = pd.DataFrame(all_vec10) # creating df to save time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "source": [
    "avg_df_10.to_csv(os.path.join(sys.path[0],'avg_df_10_rnn.csv',index=False))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "source": [
    "avg_df_10 = pd.read_csv(os.path.join(sys.path[0],'avg_df_10_rnn.csv'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "source": [
    "avg_df_10.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003418</td>\n",
       "      <td>-0.626953</td>\n",
       "      <td>0.034790</td>\n",
       "      <td>0.238159</td>\n",
       "      <td>-0.107544</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.147217</td>\n",
       "      <td>-0.387207</td>\n",
       "      <td>0.047363</td>\n",
       "      <td>0.409424</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.344360</td>\n",
       "      <td>0.261475</td>\n",
       "      <td>-0.815430</td>\n",
       "      <td>-0.156250</td>\n",
       "      <td>0.116211</td>\n",
       "      <td>-0.208496</td>\n",
       "      <td>-0.137695</td>\n",
       "      <td>-0.641602</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>-0.305054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.075195</td>\n",
       "      <td>0.237305</td>\n",
       "      <td>-0.027832</td>\n",
       "      <td>0.275879</td>\n",
       "      <td>-0.035645</td>\n",
       "      <td>-0.099609</td>\n",
       "      <td>0.324707</td>\n",
       "      <td>-0.307861</td>\n",
       "      <td>0.020142</td>\n",
       "      <td>0.284668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.239136</td>\n",
       "      <td>0.307861</td>\n",
       "      <td>-0.560059</td>\n",
       "      <td>0.272461</td>\n",
       "      <td>-0.229523</td>\n",
       "      <td>0.307373</td>\n",
       "      <td>0.311035</td>\n",
       "      <td>-0.108093</td>\n",
       "      <td>0.177734</td>\n",
       "      <td>-0.134655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.936523</td>\n",
       "      <td>0.699829</td>\n",
       "      <td>-0.291138</td>\n",
       "      <td>0.651245</td>\n",
       "      <td>-0.745911</td>\n",
       "      <td>1.101868</td>\n",
       "      <td>0.901489</td>\n",
       "      <td>-1.258240</td>\n",
       "      <td>0.322388</td>\n",
       "      <td>1.177856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299866</td>\n",
       "      <td>0.428772</td>\n",
       "      <td>-0.021408</td>\n",
       "      <td>0.895020</td>\n",
       "      <td>-0.036194</td>\n",
       "      <td>0.251877</td>\n",
       "      <td>0.784302</td>\n",
       "      <td>-1.161377</td>\n",
       "      <td>-0.026367</td>\n",
       "      <td>-0.225830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.332153</td>\n",
       "      <td>0.173981</td>\n",
       "      <td>-0.652832</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>-0.274902</td>\n",
       "      <td>0.045776</td>\n",
       "      <td>-0.187927</td>\n",
       "      <td>-0.804199</td>\n",
       "      <td>0.602280</td>\n",
       "      <td>0.339539</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.380859</td>\n",
       "      <td>0.754791</td>\n",
       "      <td>-0.224609</td>\n",
       "      <td>0.903931</td>\n",
       "      <td>-0.015869</td>\n",
       "      <td>-0.315674</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.500488</td>\n",
       "      <td>0.601074</td>\n",
       "      <td>0.224976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.089355</td>\n",
       "      <td>1.164703</td>\n",
       "      <td>0.742264</td>\n",
       "      <td>0.346519</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.715302</td>\n",
       "      <td>-0.136902</td>\n",
       "      <td>-1.475952</td>\n",
       "      <td>0.954590</td>\n",
       "      <td>0.164551</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.844757</td>\n",
       "      <td>1.655396</td>\n",
       "      <td>-1.640381</td>\n",
       "      <td>-0.247070</td>\n",
       "      <td>-0.727966</td>\n",
       "      <td>-0.047157</td>\n",
       "      <td>-0.287521</td>\n",
       "      <td>-0.253410</td>\n",
       "      <td>-0.564453</td>\n",
       "      <td>-0.148682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.003418 -0.626953  0.034790  0.238159 -0.107544  0.007812  0.147217   \n",
       "1 -0.075195  0.237305 -0.027832  0.275879 -0.035645 -0.099609  0.324707   \n",
       "2  0.936523  0.699829 -0.291138  0.651245 -0.745911  1.101868  0.901489   \n",
       "3 -0.332153  0.173981 -0.652832  0.993103 -0.274902  0.045776 -0.187927   \n",
       "4  0.089355  1.164703  0.742264  0.346519  0.001587  0.715302 -0.136902   \n",
       "\n",
       "          7         8         9  ...       290       291       292       293  \\\n",
       "0 -0.387207  0.047363  0.409424  ... -0.344360  0.261475 -0.815430 -0.156250   \n",
       "1 -0.307861  0.020142  0.284668  ... -0.239136  0.307861 -0.560059  0.272461   \n",
       "2 -1.258240  0.322388  1.177856  ... -0.299866  0.428772 -0.021408  0.895020   \n",
       "3 -0.804199  0.602280  0.339539  ... -0.380859  0.754791 -0.224609  0.903931   \n",
       "4 -1.475952  0.954590  0.164551  ... -0.844757  1.655396 -1.640381 -0.247070   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0  0.116211 -0.208496 -0.137695 -0.641602  0.183594 -0.305054  \n",
       "1 -0.229523  0.307373  0.311035 -0.108093  0.177734 -0.134655  \n",
       "2 -0.036194  0.251877  0.784302 -1.161377 -0.026367 -0.225830  \n",
       "3 -0.015869 -0.315674  0.033203  0.500488  0.601074  0.224976  \n",
       "4 -0.727966 -0.047157 -0.287521 -0.253410 -0.564453 -0.148682  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 262
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "# split for binary classification using concatenated vectors on pre trained model\n",
    "y2_nn_4b = cleaned_data[\"bin_class\"]\n",
    "X_train2_nn_4b, X_test2_nn_4b, y_train2_nn_4b, y_test2_nn_4b = train_test_split(avg_df_10, y2_nn_4b, test_size=0.2, random_state=42, stratify = y2_nn_4b)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "class FF_NL_4b(nn.Module):\n",
    "    def __init__(self, input_val, hidden_v1, hidden_v2, output_val):\n",
    "        \n",
    "        super(FF_NL_4b, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.fcl1 = nn.Linear(input_val, hidden_v1) \n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Linear function 2: 500 --> 500\n",
    "        self.fcl2 = nn.Linear(hidden_v1, hidden_v2)\n",
    "        # Non-linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Linear function 3 (readout): 500 --> 3\n",
    "        self.fcl3 = nn.Linear(hidden_v2, output_val)  \n",
    "\n",
    "    def forward(self, x_val):\n",
    "        # Linear function 1\n",
    "        out_val = self.fcl1(x_val)\n",
    "        out_val = self.relu1(out_val)\n",
    "        out_val = self.fcl2(out_val)\n",
    "        out_val = self.relu2(out_val)\n",
    "        out_val = self.fcl3(out_val)\n",
    "\n",
    "        return torch.sigmoid(out_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "model = FF_NL_4b(300,50,10,2)\n",
    "x = torch.randn(50,300)\n",
    "print(model(x).shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([50, 2])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "# initialisng variables\n",
    "input_dim = 300\n",
    "hidden_dim1 = 50\n",
    "hidden_dim2 =10\n",
    "output_dim = 2\n",
    "num_epochs = 10\n",
    "\n",
    "ff_nn_w2v_my_model_4b = FF_NL(input_dim, hidden_dim1,hidden_dim2, output_dim)\n",
    "\n",
    "loss_fn_est_4b = nn.CrossEntropyLoss()\n",
    "optimizer_4b = optim.Adam(ff_nn_w2v_my_model_4b.parameters(), lr=0.001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "source": [
    "ff_nn_w2v_my_model_4b.train()\n",
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    loss_train_4b = 0\n",
    "    \n",
    "    for idx in range(len(X_train2_nn_4b)):\n",
    "        \n",
    "        x_inp_4b = X_train2_nn_4b.iloc[idx]\n",
    "        x_inp_4b = torch.tensor(x_inp_4b)\n",
    "        x_inp_4b = torch.reshape(x_inp_4b,(1,300)).float()\n",
    "        \n",
    "        x_prob_4b = ff_nn_w2v_my_model_4b(x_inp_4b)\n",
    "        x_prob_4b = torch.reshape(x_prob_4b,(1,2))\n",
    "        \n",
    "        y_inp_4b = torch.tensor([y_train2_nn_4b.iloc[idx]])\n",
    "        \n",
    "        optimizer_4b.zero_grad()\n",
    "        loss_4b = loss_fn_est_4b(x_prob_4b, y_inp_4b)\n",
    "        \n",
    "        loss_train_4b += loss_4b.item()\n",
    "        loss_4b.backward()\n",
    "\n",
    "        optimizer_4b.step()\n",
    "    \n",
    "    print('Epoch:', (epoch + 1), 'loss =', '{:.6f}'.format((loss_train_4b / len(X_train2_nn_4b))))\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1 loss = 0.505935\n",
      "Epoch: 2 loss = 0.503346\n",
      "Epoch: 3 loss = 0.500717\n",
      "Epoch: 4 loss = 0.498564\n",
      "Epoch: 5 loss = 0.497481\n",
      "Epoch: 6 loss = 0.497930\n",
      "Epoch: 7 loss = 0.498317\n",
      "Epoch: 8 loss = 0.497262\n",
      "Epoch: 9 loss = 0.499047\n",
      "Epoch: 10 loss = 0.495007\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "source": [
    "# finding report on test split\n",
    "from sklearn.metrics import classification_report\n",
    "test_pred = []\n",
    "orig_label = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(X_test2_nn_4b)):\n",
    "        \n",
    "        cxv = X_test2_nn_4b.iloc[idx]\n",
    "        cxv = torch.tensor(cxv)\n",
    "        cxv = torch.reshape(cxv,(1,300)).float()\n",
    "        probs_t = ff_nn_w2v_my_model_4b(cxv)\n",
    "        vcx = torch.tensor([y_test2_nn_4b.iloc[idx]])\n",
    "        probs_t = torch.reshape(probs_t,(1,2))\n",
    "        test_pred.append(torch.argmax(probs_t, dim=1).numpy()[0])\n",
    "        orig_label.append(vcx.numpy()[0])\n",
    "print(classification_report(orig_label,test_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.84      0.79      8000\n",
      "           1       0.81      0.71      0.76      8000\n",
      "\n",
      "    accuracy                           0.77     16000\n",
      "   macro avg       0.78      0.77      0.77     16000\n",
      "weighted avg       0.78      0.77      0.77     16000\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "# loading cleaned data which has all the classes i.e neutral class as well\n",
    "cleaned_data_multi = pd.read_csv(os.path.join(sys.path[0],'cleaned_data.csv'))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "cleaned_data_multi=cleaned_data_multi.dropna() # Dropping reviews that have type NaN as this will cause when randomly selecting classes with a particular class\n",
    "cleaned_data_multi = cleaned_data_multi.reset_index(drop=True) # reseting the index tso that it covers up for the dropped values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "cleaned_data_multi.head()\n",
    "(cleaned_data_multi['bin_class'].value_counts())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    99978\n",
       "1    99921\n",
       "3    49974\n",
       "Name: bin_class, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "# I am selecting a smaller dataset due to hardware limitations\n",
    "cleaned_data_multi['bin_class']=cleaned_data_multi['bin_class'].astype(int)\n",
    "s1 = cleaned_data_multi.bin_class[cleaned_data_multi.bin_class.eq(0)].sample(30000,random_state=1).index\n",
    "s2 = cleaned_data_multi.bin_class[cleaned_data_multi.bin_class.eq(1)].sample(30000,random_state=1).index \n",
    "s3 = cleaned_data_multi.bin_class[cleaned_data_multi.bin_class.eq(3)].sample(30000,random_state=1).index \n",
    "\n",
    "cleaned_data_multi = cleaned_data_multi.loc[s1.union(s2).union(s3)]\n",
    "display(cleaned_data_multi['bin_class'].value_counts())\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "3    30000\n",
       "1    30000\n",
       "0    30000\n",
       "Name: bin_class, dtype: int64"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "cleaned_data_multi['tokens'] = cleaned_data_multi['review_body'].apply(word_tokenize)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "cleaned_data_multi.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>bin_class</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>little awkward use great concept depending ski...</td>\n",
       "      <td>3</td>\n",
       "      <td>[little, awkward, use, great, concept, dependi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>bought boyfriend loved</td>\n",
       "      <td>1</td>\n",
       "      <td>[bought, boyfriend, loved]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>149</td>\n",
       "      <td>3</td>\n",
       "      <td>work dent</td>\n",
       "      <td>3</td>\n",
       "      <td>[work, dent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>165</td>\n",
       "      <td>3</td>\n",
       "      <td>fist set cracked ok</td>\n",
       "      <td>3</td>\n",
       "      <td>[fist, set, cracked, ok]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>275</td>\n",
       "      <td>1</td>\n",
       "      <td>product work got</td>\n",
       "      <td>0</td>\n",
       "      <td>[product, work, got]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  star_rating  \\\n",
       "0           75            3   \n",
       "3          103            4   \n",
       "5          149            3   \n",
       "6          165            3   \n",
       "11         275            1   \n",
       "\n",
       "                                          review_body  bin_class  \\\n",
       "0   little awkward use great concept depending ski...          3   \n",
       "3                              bought boyfriend loved          1   \n",
       "5                                           work dent          3   \n",
       "6                                 fist set cracked ok          3   \n",
       "11                                   product work got          0   \n",
       "\n",
       "                                               tokens  \n",
       "0   [little, awkward, use, great, concept, dependi...  \n",
       "3                          [bought, boyfriend, loved]  \n",
       "5                                        [work, dent]  \n",
       "6                            [fist, set, cracked, ok]  \n",
       "11                               [product, work, got]  "
      ]
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "# calling method to find avg of vectors which will also have neutral review vectors\n",
    "all_avg_vec_multi = averageVector_wordmodel(cleaned_data_multi['tokens'], wordmodel_new)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "avg_df_multi = pd.DataFrame(all_avg_vec_multi) # converting to df to save time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "avg_df_multi.to_csv(os.path.join(sys.path[0],'avg_vector_word_multi.csv',index=False)) # saving as csv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "ret_avg_df_word_multi = pd.read_csv(os.path.join(sys.path[0],'avg_vector_word_multi.csv')) # loading file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "# Class defintion of Feedforward Neural Network with 2 hidden layers multiclass classification\n",
    "class FF_NL_multi(nn.Module):\n",
    "    def __init__(self, input_val, hidden_v1, hidden_v2, output_val):\n",
    "        \n",
    "        super(FF_NL_multi, self).__init__() \n",
    "        self.fcl1 = nn.Linear(input_val, hidden_v1) \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fcl2 = nn.Linear(hidden_v1, hidden_v2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fcl3 = nn.Linear(hidden_v2, output_val)  \n",
    "\n",
    "    def forward(self, x_val):\n",
    "\n",
    "        out_val = self.fcl1(x_val)\n",
    "        out_val = self.relu1(out_val)\n",
    "        out_val = self.fcl2(out_val)\n",
    "        out_val = self.relu2(out_val)\n",
    "        out_val = self.fcl3(out_val)\n",
    "\n",
    "        return torch.softmax(out_val,dim=1) # softmax for ternery classification"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "# split for multiclass classification using Word2Vec vectors on pre trained model\n",
    "y2_nn_multi = cleaned_data_multi[\"bin_class\"]\n",
    "X_train2_nn_multi, X_test2_nn_multi, y_train2_nn_multi, y_test2_nn_multi = train_test_split(ret_avg_df_word_multi, y2_nn_multi, test_size=0.2, random_state=42, stratify = y2_nn_multi)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "source": [
    "# initialising variables \n",
    "input_dim = 300\n",
    "hidden_dim1 = 50\n",
    "hidden_dim2 =10\n",
    "output_dim = 3  # 3 for neutral class\n",
    "num_epochs = 50\n",
    "\n",
    "ff_nn_w2v_my_model_multi = FF_NL_multi(input_dim, hidden_dim1,hidden_dim2, output_dim)\n",
    "\n",
    "loss_fn_est_multi = nn.CrossEntropyLoss()\n",
    "optimizer_multi = optim.Adam(ff_nn_w2v_my_model_multi.parameters(), lr=0.0001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "source": [
    "# method to return torch tensor for target class 3, since we have class value as 3 and pytorch takes linear class values as 0,1,2 and so on\n",
    "def modify_y_inp(label):\n",
    "    if label[0] == 0:\n",
    "        return torch.tensor([0])\n",
    "    elif label[0] == 1:\n",
    "        return torch.tensor([1])\n",
    "    else:\n",
    "        return torch.tensor([2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "source": [
    "ff_nn_w2v_my_model_multi.train()\n",
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    loss_train_multi = 0\n",
    "    \n",
    "    for idx in range(len(X_train2_nn_multi)):\n",
    "        \n",
    "        x_inp = X_train2_nn_multi.iloc[idx]\n",
    "        x_inp = torch.tensor(x_inp)\n",
    "        x_inp = torch.reshape(x_inp,(1,300)).float()\n",
    "    \n",
    "        x_prob = ff_nn_w2v_my_model_multi(x_inp)\n",
    "        x_prob = torch.reshape(x_prob,(1,3))\n",
    "        \n",
    "        y_inp = modify_y_inp([y_train2_nn_multi.iloc[idx]])\n",
    "        \n",
    "        optimizer_multi.zero_grad()\n",
    "        loss_multi = loss_fn_est_multi(x_prob, y_inp)\n",
    "        \n",
    "        loss_train_multi += loss_multi.item()\n",
    "        loss_multi.backward()\n",
    "\n",
    "        optimizer_multi.step()\n",
    "       \n",
    "    print('Epoch:', (epoch + 1), 'loss =', '{:.6f}'.format((loss_train_multi / len(X_train2_nn_multi))))\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1 loss = 0.854068\n",
      "Epoch: 2 loss = 0.852083\n",
      "Epoch: 3 loss = 0.851135\n",
      "Epoch: 4 loss = 0.850527\n",
      "Epoch: 5 loss = 0.849631\n",
      "Epoch: 6 loss = 0.849350\n",
      "Epoch: 7 loss = 0.848820\n",
      "Epoch: 8 loss = 0.847964\n",
      "Epoch: 9 loss = 0.847694\n",
      "Epoch: 10 loss = 0.846760\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# finding report on test split\n",
    "from sklearn.metrics import classification_report\n",
    "test_pred = []\n",
    "orig_label = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(X_test2_nn_multi)):\n",
    "        \n",
    "        cxv = X_test2_nn_multi.iloc[idx]\n",
    "        cxv = torch.tensor(cxv)\n",
    "        cxv = torch.reshape(cxv,(1,300)).float()\n",
    "        probs_t = ff_nn_w2v_my_model_multi(cxv)\n",
    "        vcx = modify_y_inp([y_test2_nn_multi.iloc[idx]])\n",
    "        probs_t = torch.reshape(probs_t,(1,3))\n",
    "        test_pred.append(torch.argmax(probs_t, dim=1).numpy()[0])\n",
    "        orig_label.append(vcx.numpy()[0])\n",
    "print(classification_report(orig_label,test_pred))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "source": [
    "all_vec10_multi = concat_w2v(cleaned_data_multi['tokens'], wordmodel_new) # calling the concat method with neutral class as well"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "source": [
    "avg_df_10_multi = pd.DataFrame(all_vec10_multi) # creating df to save time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "source": [
    "avg_df_10_multi.to_csv(os.path.join(sys.path[0],'avg_df_rnn_word.csv',index=False))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "source": [
    "avg_df_10_multi = pd.read_csv(os.path.join(sys.path[0],'avg_df_rnn_word.csv'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "source": [
    "# split for ternary classification using concatenated vectors on pre trained model\n",
    "y2_nn_4b_multi  = cleaned_data_multi[\"bin_class\"]\n",
    "X_train2_nn_4b_multi , X_test2_nn_4b_multi , y_train2_nn_4b_multi , y_test2_nn_4b_multi  = train_test_split(avg_df_10_multi, y2_nn_4b_multi , test_size=0.2, random_state=8, stratify = y2_nn_4b_multi)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "source": [
    "# initialising variables \n",
    "input_dim = 300\n",
    "hidden_dim1 = 50\n",
    "hidden_dim2 =10\n",
    "output_dim = 3 # 3 for neutral class\n",
    "num_epochs = 10\n",
    "\n",
    "ff_nn_w2v_my_model_4b_multi  = FF_NL_multi(input_dim, hidden_dim1,hidden_dim2, output_dim)\n",
    "\n",
    "loss_fn_est_4b_multi = nn.CrossEntropyLoss()\n",
    "optimizer_4b_multi = optim.Adam(ff_nn_w2v_my_model_4b_multi.parameters(), lr=0.00001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "source": [
    "ff_nn_w2v_my_model_4b_multi.train()\n",
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    loss_train_4b_multi = 0\n",
    "    \n",
    "    for idx in range(len(X_train2_nn_4b_multi)):\n",
    "        \n",
    "        x_inp_4b = X_train2_nn_4b_multi.iloc[idx]\n",
    "        x_inp_4b = torch.tensor(x_inp_4b)\n",
    "        x_inp_4b = torch.reshape(x_inp_4b,(1,300)).float()\n",
    "        \n",
    "        x_prob_4b = ff_nn_w2v_my_model_4b_multi(x_inp_4b)\n",
    "        x_prob_4b = torch.reshape(x_prob_4b,(1,3))\n",
    "        \n",
    "        y_inp_4b = make_target([y_train2_nn_4b_multi.iloc[idx]])\n",
    "        \n",
    "        optimizer_4b_multi.zero_grad()\n",
    "        loss_4b_multi = loss_fn_est_4b_multi(x_prob_4b, y_inp_4b)\n",
    "        loss_train_4b_multi += loss_4b_multi.item()\n",
    "\n",
    "        loss_4b_multi.backward()\n",
    "\n",
    "        optimizer_4b_multi.step()\n",
    "    \n",
    "    \n",
    "    print('Epoch:', (epoch + 1), 'loss =', '{:.6f}'.format((loss_train_4b_multi / len(X_train2_nn_4b_multi))))\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1 loss = 1.052076\n",
      "Epoch: 2 loss = 0.997541\n",
      "Epoch: 3 loss = 0.983895\n",
      "Epoch: 4 loss = 0.977628\n",
      "Epoch: 5 loss = 0.973853\n",
      "Epoch: 6 loss = 0.971257\n",
      "Epoch: 7 loss = 0.969320\n",
      "Epoch: 8 loss = 0.967803\n",
      "Epoch: 9 loss = 0.966570\n",
      "Epoch: 10 loss = 0.965547\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "source": [
    "# finding report on test split\n",
    "from sklearn.metrics import classification_report\n",
    "test_pred = []\n",
    "orig_label = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(X_test2_nn_4b_multi)):\n",
    "        \n",
    "        cxv = X_test2_nn_4b_multi.iloc[idx]\n",
    "        cxv = torch.tensor(cxv)\n",
    "        cxv = torch.reshape(cxv,(1,300)).float()\n",
    "        probs_t = ff_nn_w2v_my_model_4b_multi(cxv)\n",
    "        vcx = modify_y_inp([y_test2_nn_4b_multi.iloc[idx]])\n",
    "        probs_t = torch.reshape(probs_t,(1,3))\n",
    "        test_pred.append(torch.argmax(probs_t, dim=1).numpy()[0])\n",
    "        orig_label.append(vcx.numpy()[0])\n",
    "print(classification_report(orig_label,test_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.65      0.60      6000\n",
      "           1       0.63      0.60      0.62      6000\n",
      "           2       0.50      0.44      0.47      6000\n",
      "\n",
      "    accuracy                           0.56     18000\n",
      "   macro avg       0.56      0.56      0.56     18000\n",
      "weighted avg       0.56      0.56      0.56     18000\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "source": [
    "cleaned_data_rnn = pd.read_csv(os.path.join(sys.path[0],'cleaned_data.csv')) # load cleaned reviews \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "source": [
    "cleaned_data_rnn=cleaned_data_rnn.dropna() # Dropping reviews that have type NaN as this will cause when randomly selecting classes with a particular class\n",
    "cleaned_data_rnn = cleaned_data_rnn.reset_index(drop=True) # reseting the index tso that it covers up for the dropped values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "source": [
    "cleaned_data_rnn = cleaned_data_rnn.loc[cleaned_data_rnn[\"bin_class\"] != 3] # remove neutral reviews"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "source": [
    "cleaned_data_rnn.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>bin_class</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>blender look like new centering pad</td>\n",
       "      <td>1</td>\n",
       "      <td>['Our', 'blender', 'looks', 'like', 'new', 'wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>bought boyfriend loved</td>\n",
       "      <td>1</td>\n",
       "      <td>['Bought', 'it', 'for', 'my', 'boyfriend', 'he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>124</td>\n",
       "      <td>4</td>\n",
       "      <td>like dish color clear redcranberry color swirl...</td>\n",
       "      <td>1</td>\n",
       "      <td>['What', 'I', 'like', 'about', 'the', 'dishes'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>254</td>\n",
       "      <td>1</td>\n",
       "      <td>loud alarm loud allvery disappointedrather spe...</td>\n",
       "      <td>0</td>\n",
       "      <td>['This', '&amp;', '#', '34', ';', 'LOUD', '&amp;', '#'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>275</td>\n",
       "      <td>1</td>\n",
       "      <td>product work got</td>\n",
       "      <td>0</td>\n",
       "      <td>['My', 'product', 'did', \"n't\", 'work', 'when'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  star_rating  \\\n",
       "2          100            4   \n",
       "3          103            4   \n",
       "4          124            4   \n",
       "10         254            1   \n",
       "11         275            1   \n",
       "\n",
       "                                          review_body  bin_class  \\\n",
       "2                 blender look like new centering pad          1   \n",
       "3                              bought boyfriend loved          1   \n",
       "4   like dish color clear redcranberry color swirl...          1   \n",
       "10  loud alarm loud allvery disappointedrather spe...          0   \n",
       "11                                   product work got          0   \n",
       "\n",
       "                                               tokens  \n",
       "2   ['Our', 'blender', 'looks', 'like', 'new', 'wi...  \n",
       "3   ['Bought', 'it', 'for', 'my', 'boyfriend', 'he...  \n",
       "4   ['What', 'I', 'like', 'about', 'the', 'dishes'...  \n",
       "10  ['This', '&', '#', '34', ';', 'LOUD', '&', '#'...  \n",
       "11  ['My', 'product', 'did', \"n't\", 'work', 'when'...  "
      ]
     },
     "metadata": {},
     "execution_count": 220
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "source": [
    "# I am selecting a smaller dataset due to hardware limitations\n",
    "cleaned_data_rnn['bin_class']=cleaned_data_rnn['bin_class'].astype(int)\n",
    "s1 = cleaned_data_rnn.bin_class[cleaned_data_rnn.bin_class.eq(0)].sample(40000,random_state=1).index\n",
    "s2 = cleaned_data_rnn.bin_class[cleaned_data_rnn.bin_class.eq(1)].sample(40000,random_state=1).index \n",
    "\n",
    "\n",
    "cleaned_data_rnn = cleaned_data_rnn.loc[s1.union(s2)]\n",
    "display(cleaned_data_rnn['bin_class'].value_counts())\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "1    40000\n",
       "0    40000\n",
       "Name: bin_class, dtype: int64"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "source": [
    "# finding the average of each review vectors\n",
    "def process_inp_rnn(tokens, w2v):\n",
    "    all_avg = []\n",
    "    x = w2v.index_to_key\n",
    "    df =0\n",
    "    for i in tokens:\n",
    "        \n",
    "        vec_avg = []\n",
    "        max_len = 0\n",
    "        flag = False\n",
    "        for word in i:\n",
    "            #print('word',word)\n",
    "            if max_len==50:\n",
    "                flag = True\n",
    "                break\n",
    "            if word in x:\n",
    "                max_len+=1\n",
    "                vec_avg.append(w2v[word])\n",
    "                #print('vec',vec_avg)\n",
    "                \n",
    "        if not flag:\n",
    "            rem = 50 - max_len\n",
    "            #print('rem',rem)\n",
    "            for s in range(rem):\n",
    "                vec_avg.append([0]*300)\n",
    "            #print('vec1',vec_avg) \n",
    "        #df+=1\n",
    "        \n",
    "        all_avg.append(vec_avg)\n",
    "        #print('all',all_avg) \n",
    "    return all_avg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "allVectors_rnn = process_inp_rnn(cleaned_data_rnn['tokens'], wordmodel_new) #calling the method to find avg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_rnn = pd.DataFrame(allVectors_rnn) # creating df to save time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_rnn.to_csv(os.path.join(sys.path[0],'all_rnn_word.csv',index=False)) #saving avg vectors for reproducibilty "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "source": [
    "ret_all_rnn= pd.read_csv(os.path.join(sys.path[0],'avg_df_10_rnn.csv')) # loading it"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "source": [
    "# split for binary classification using concatenated vectors on pre trained model\n",
    "from sklearn.model_selection import train_test_split\n",
    "y2_nn_rnn = cleaned_data_rnn[\"bin_class\"]\n",
    "X_train2_rnn, X_test2_rnn, y_train2_rnn, y_test2_rnn = train_test_split(ret_all_rnn, y2_nn_rnn, test_size=0.2, random_state=16, stratify = y2_nn_rnn)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "source": [
    "class Google_RNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Google_RNN, self).__init__()\n",
    "        self.num_lay = 1\n",
    "        self.size_hid = 50\n",
    "        self.my_rnn = torch.nn.RNN(300, self.size_hid, self.num_lay)\n",
    "        self.fc = torch.nn.Linear(self.size_hid, 1)\n",
    "        self.drop = torch.nn.Dropout(0.4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_len = x.size()\n",
    "        hid_val = self.init_hidden(batch_len)\n",
    "        out, hid_val = self.my_rnn(x, hid_val)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_len):\n",
    "        hid_val = torch.zeros(self.num_lay, batch_len, self.size_hid)\n",
    "        #print('hee')\n",
    "        return hid_val\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "source": [
    "num_epochs = 10\n",
    "rnn_my_model = Google_RNN()\n",
    "print(rnn_my_model)\n",
    "rnn_google_model = FF_NL(input_dim, hidden_dim1,hidden_dim2, output_dim)\n",
    "loss_fn_rnn = torch.nn.CrossEntropyLoss()\n",
    "optimizer_rnn = torch.optim.Adam(rnn_my_model.parameters(), lr = 0.0001)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "My_RNN(\n",
      "  (my_rnn): RNN(300, 50)\n",
      "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (drop): Dropout(p=0.4, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "source": [
    "rnn_google_model.train()\n",
    "\n",
    "# Start training\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #print('hh')\n",
    "    loss_train_rnn = 0\n",
    "    \n",
    "    for idx in range(len(X_train2_rnn)):\n",
    "        #print('le',len(X_train2_rnn))\n",
    "        x_inp_rnn = X_train2_rnn.iloc[idx]\n",
    "        #print(x_inp_rnn)\n",
    "        x_inp_rnn = torch.tensor(x_inp_rnn)\n",
    "        x_inp_rnn = torch.reshape(x_inp_rnn,(1,300)).float()\n",
    "        #print(x_inp_rnn)\n",
    "        x_prob_rnn = rnn_google_model(x_inp_rnn)\n",
    "        x_prob_rnn = torch.reshape(x_prob_rnn,(1,2))\n",
    "        \n",
    "        y_inp_rnn = torch.tensor([y_train2_rnn.iloc[idx]])\n",
    "        \n",
    "        optimizer_rnn.zero_grad()\n",
    "        loss_rnn = loss_fn_rnn(x_prob_rnn, y_inp_rnn)\n",
    "        \n",
    "        loss_train_rnn += loss_rnn.item()\n",
    "        loss_rnn.backward()\n",
    "\n",
    "        optimizer_rnn.step()\n",
    "    \n",
    "    print('Epoch:',  (epoch + 1), 'loss =', '{:.6f}'.format((loss_train_rnn / len(X_train2_rnn))))\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1 loss = 0.558783\n",
      "Epoch: 2 loss = 0.539190\n",
      "Epoch: 3 loss = 0.534232\n",
      "Epoch: 4 loss = 0.530159\n",
      "Epoch: 5 loss = 0.526245\n",
      "Epoch: 6 loss = 0.522152\n",
      "Epoch: 7 loss = 0.518018\n",
      "Epoch: 8 loss = 0.513964\n",
      "Epoch: 9 loss = 0.510155\n",
      "Epoch: 10 loss = 0.506666\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# finding report on test split\n",
    "from sklearn.metrics import classification_report\n",
    "test_pred = []\n",
    "orig_label = []\n",
    "tp = 0\n",
    "sa = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(X_test2_rnn)):\n",
    "        \n",
    "        cxv = X_test2_rnn.iloc[idx]\n",
    "        cxv = torch.tensor(cxv)\n",
    "        cxv = torch.reshape(cxv,(1,300)).float()\n",
    "        probs_t = rnn_google_model(cxv)\n",
    "        vcx = torch.tensor([y_test2_rnn.iloc[idx]])\n",
    "        probs_t = torch.reshape(probs_t,(1,2))\n",
    "        test_pred.append(torch.argmax(probs_t, dim=1).numpy()[0])\n",
    "        orig_label.append(vcx.numpy()[0])\n",
    "print(classification_report(orig_label,test_pred))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "allVectors_rnn_multi = process_inp_rnn(cleaned_data_rnn['tokens'], My_model_trained) #calling the method to find avg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rnn_df_multi = pd.DataFrame(allVectors_rnn_multi) # converting to df to save time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rnn_df_multi.to_csv(os.path.join(sys.path[0],'rnn_vector_multi_word.csv',index=False)) # saving as csv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "source": [
    "ret_rnn_multi = pd.read_csv(os.path.join(sys.path[0],'avg_df_rnn_word.csv')) # loading file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "source": [
    "# split for multiclass classification using Word2Vec vectors on my trained model\n",
    "y2_rnn_multi = cleaned_data_multi[\"bin_class\"]\n",
    "X_train2_rnn_multi, X_test2_rnn_multi, y_train2_rnn_multi, y_test2_rnn_multi = train_test_split(ret_rnn_multi, y2_rnn_multi, test_size=0.2, random_state=32, stratify = y2_rnn_multi)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "source": [
    "class Google_RNN_multi(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Google_RNN_multi, self).__init__()\n",
    "        self.num_lay = 1\n",
    "        self.size_hid = 50\n",
    "        self.my_rnn = torch.nn.RNN(300, self.size_hid, self.num_lay)\n",
    "        self.fc = torch.nn.Linear(self.size_hid, 1)\n",
    "        self.drop = torch.nn.Dropout(0.4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_len = x.size()\n",
    "        hid_val = self.init_hidden(batch_len)\n",
    "        out, hid_val = self.my_rnn(x, hid_val)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_len):\n",
    "        hid_val = torch.zeros(self.num_lay, batch_len, self.size_hid)\n",
    "        #print('hee')\n",
    "        return hid_val\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "source": [
    "num_epochs = 10\n",
    "rnn_google_model_multi = Google_RNN_multi()\n",
    "print(rnn_google_model_multi)\n",
    "rnn_google_model_multi = FF_NL_multi(input_dim, hidden_dim1,hidden_dim2, output_dim)\n",
    "loss_fn_rnn_multi = torch.nn.CrossEntropyLoss()\n",
    "optimizer_rnn_multi = torch.optim.Adam(rnn_my_model.parameters(), lr = 0.0001)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Google_RNN_multi(\n",
      "  (my_rnn): RNN(300, 50)\n",
      "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (drop): Dropout(p=0.4, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "source": [
    "rnn_google_model_multi.train()\n",
    "\n",
    "# Start training\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #print('hh')\n",
    "    loss_train_rnn = 0\n",
    "    \n",
    "    for idx in range(len(X_train2_rnn_multi)):\n",
    "        #print('le',len(X_train2_rnn))\n",
    "        x_inp_rnn = X_train2_nn_multi.iloc[idx]\n",
    "        #print(x_inp_rnn)\n",
    "        x_inp_rnn = torch.tensor(x_inp_rnn)\n",
    "        x_inp_rnn = torch.reshape(x_inp_rnn,(1,300)).float()\n",
    "        #print(x_inp_rnn)\n",
    "        x_prob_rnn = rnn_google_model_multi(x_inp_rnn)\n",
    "        x_prob_rnn = torch.reshape(x_prob_rnn,(1,3))\n",
    "        \n",
    "        y_inp_rnn = modify_y_inp([y_train2_rnn_multi.iloc[idx]])\n",
    "        \n",
    "        optimizer_rnn_multi.zero_grad()\n",
    "        loss_rnn = loss_fn_rnn_multi(x_prob_rnn, y_inp_rnn)\n",
    "        \n",
    "        loss_train_rnn += loss_rnn.item()\n",
    "        loss_rnn.backward()\n",
    "\n",
    "        optimizer_rnn_multi.step()\n",
    "    \n",
    "    print('Epoch:',  (epoch + 1), 'loss =', '{:.6f}'.format((loss_train_rnn / len(X_train2_rnn_multi))))\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1 loss = 1.101173\n",
      "Epoch: 2 loss = 1.101173\n",
      "Epoch: 3 loss = 1.101173\n",
      "Epoch: 4 loss = 1.101173\n",
      "Epoch: 5 loss = 1.101173\n",
      "Epoch: 6 loss = 1.101173\n",
      "Epoch: 7 loss = 1.101173\n",
      "Epoch: 8 loss = 1.101173\n",
      "Epoch: 9 loss = 1.101173\n",
      "Epoch: 10 loss = 1.101173\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# finding report on test split\n",
    "from sklearn.metrics import classification_report\n",
    "test_pred = []\n",
    "orig_label = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(X_test2_rnn_multi)):\n",
    "        \n",
    "        cxv = X_test2_rnn_multi.iloc[idx]\n",
    "        cxv = torch.tensor(cxv)\n",
    "        cxv = torch.reshape(cxv,(1,300)).float()\n",
    "        probs_t = rnn_google_model_multi(cxv)\n",
    "        vcx = torch.tensor([y_test2_rnn_multi.iloc[idx]])\n",
    "        probs_t = torch.reshape(probs_t,(1,3))\n",
    "        test_pred.append(torch.argmax(probs_t, dim=1).numpy()[0])\n",
    "        orig_label.append(vcx.numpy()[0])\n",
    "print(classification_report(orig_label,test_pred))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class My_GRU(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(My_GRU, self).__init__()\n",
    "        self.num_lay = 1\n",
    "        self.size_hid = 50\n",
    "        self.my_rnn = torch.nn.GRU(300, self.size_hid, self.num_lay)\n",
    "        self.fc = torch.nn.Linear(self.size_hid, 1)\n",
    "        self.drop = torch.nn.Dropout(0.4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_len = x.size()\n",
    "        hid_val = self.init_hidden(batch_len)\n",
    "        out, hid_val = self.my_rnn(x, hid_val)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_len):\n",
    "        hid_val = torch.zeros(self.num_lay, batch_len, self.size_hid)\n",
    "        #print('hee')\n",
    "        return hid_val\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ret_all_gru = pd.read_csv(os.path.join(sys.path[0],'avg_vector_word.csv'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# split for binary classification using concatenated vectors on pre trained model\n",
    "from sklearn.model_selection import train_test_split\n",
    "y2_nn_gru = cleaned_data_rnn[\"bin_class\"]\n",
    "X_train2_gru, X_test2_gru, y_train2_gru, y_test2_gru = train_test_split(ret_all_gru, y2_nn_gru, test_size=0.2, random_state=52, stratify = y2_nn_gru)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_epochs = 10\n",
    "gru_my_model = My_RNN()\n",
    "print(gru_my_model)\n",
    "gru_my_model = FF_NL(input_dim, hidden_dim1,hidden_dim2, output_dim)\n",
    "loss_fn_rnn = torch.nn.CrossEntropyLoss()\n",
    "optimizer_rnn = torch.optim.Adam(rnn_my_model.parameters(), lr = 0.0001)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gru_my_model.train()\n",
    "\n",
    "# Start training\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #print('hh')\n",
    "    loss_train_rnn = 0\n",
    "    \n",
    "    for idx in range(len(X_train2_gru)):\n",
    "        #print('le',len(X_train2_rnn))\n",
    "        x_inp_rnn = X_train2_gru.iloc[idx]\n",
    "        #print(x_inp_rnn)\n",
    "        x_inp_rnn = torch.tensor(x_inp_rnn)\n",
    "        x_inp_rnn = torch.reshape(x_inp_rnn,(1,300)).float()\n",
    "        #print(x_inp_rnn)\n",
    "        x_prob_rnn = gru_my_model(x_inp_rnn)\n",
    "        x_prob_rnn = torch.reshape(x_prob_rnn,(1,2))\n",
    "        \n",
    "        y_inp_rnn = torch.tensor([y_train2_gru.iloc[idx]])\n",
    "        \n",
    "        optimizer_rnn.zero_grad()\n",
    "        loss_rnn = loss_fn_rnn(x_prob_rnn, y_inp_rnn)\n",
    "        \n",
    "        loss_train_rnn += loss_rnn.item()\n",
    "        loss_rnn.backward()\n",
    "\n",
    "        optimizer_rnn.step()\n",
    "    \n",
    "    print('Epoch:',  (epoch + 1), 'loss =', '{:.6f}'.format((loss_train_rnn / len(X_train2_gru))))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# finding report on test split\n",
    "from sklearn.metrics import classification_report\n",
    "test_pred = []\n",
    "orig_label = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(X_test2_gru)):\n",
    "        \n",
    "        cxv = X_test2_gru.iloc[idx]\n",
    "        cxv = torch.tensor(cxv)\n",
    "        cxv = torch.reshape(cxv,(1,300)).float()\n",
    "        probs_t = gru_my_model(cxv)\n",
    "        vcx = torch.tensor([y_test2_gru.iloc[idx]])\n",
    "        probs_t = torch.reshape(probs_t,(1,2))\n",
    "        test_pred.append(torch.argmax(probs_t, dim=1).numpy()[0])\n",
    "        orig_label.append(vcx.numpy()[0])\n",
    "print(classification_report(orig_label,test_pred))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ret_gru_multi = pd.read_csv(os.path.join(sys.path[0],'avg_vector_word_multi.csv')) # loading file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# split for multiclass classification using Word2Vec vectors on my trained model\n",
    "y2_gru_multi = cleaned_data_multi[\"bin_class\"]\n",
    "X_train2_gru_multi, X_test2_gru_multi, y_train2_gru_multi, y_test2_gru_multi = train_test_split(ret_rnn_multi, y2_gru_multi, test_size=0.2, random_state=32, stratify = y2_gru_multi)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class My_GRU_multi(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(My_RNN_multi, self).__init__()\n",
    "        self.num_lay = 1\n",
    "        self.size_hid = 50\n",
    "        self.my_rnn = torch.nn.GRU(300, self.size_hid, self.num_lay)\n",
    "        self.fc = torch.nn.Linear(self.size_hid, 1)\n",
    "        self.drop = torch.nn.Dropout(0.4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_len = x.size()\n",
    "        hid_val = self.init_hidden(batch_len)\n",
    "        out, hid_val = self.my_rnn(x, hid_val)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_len):\n",
    "        hid_val = torch.zeros(self.num_lay, batch_len, self.size_hid)\n",
    "        #print('hee')\n",
    "        return hid_val\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_epochs = 10\n",
    "gru_my_model_multi = My_RNN_multi()\n",
    "print(gru_my_model_multi)\n",
    "gru_my_model_multi = FF_NL_multi(input_dim, hidden_dim1,hidden_dim2, output_dim)\n",
    "loss_fn_rnn_multi = torch.nn.CrossEntropyLoss()\n",
    "optimizer_rnn_multi = torch.optim.Adam(rnn_my_model.parameters(), lr = 0.0001)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gru_my_model_multi.train()\n",
    "\n",
    "# Start training\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #print('hh')\n",
    "    loss_train_rnn = 0\n",
    "    \n",
    "    for idx in range(len(X_train2_gru_multi)):\n",
    "        #print('le',len(X_train2_rnn))\n",
    "        x_inp_rnn = X_train2_gru_multi.iloc[idx]\n",
    "        #print(x_inp_rnn)\n",
    "        x_inp_rnn = torch.tensor(x_inp_rnn)\n",
    "        x_inp_rnn = torch.reshape(x_inp_rnn,(1,300)).float()\n",
    "        #print(x_inp_rnn)\n",
    "        x_prob_rnn = gru_my_model_multi(x_inp_rnn)\n",
    "        x_prob_rnn = torch.reshape(x_prob_rnn,(1,3))\n",
    "        \n",
    "        y_inp_rnn = modify_y_inp([y_train2_gru_multi.iloc[idx]])\n",
    "        \n",
    "        optimizer_rnn_multi.zero_grad()\n",
    "        loss_rnn = loss_fn_rnn_multi(x_prob_rnn, y_inp_rnn)\n",
    "        \n",
    "        loss_train_rnn += loss_rnn.item()\n",
    "        loss_rnn.backward()\n",
    "\n",
    "        optimizer_rnn_multi.step()\n",
    "    \n",
    "    print('Epoch:',  (epoch + 1), 'loss =', '{:.6f}'.format((loss_train_rnn / len(X_train2_gru_multi))))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# finding report on test split\n",
    "from sklearn.metrics import classification_report\n",
    "test_pred = []\n",
    "orig_label = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(X_test2_gru_multi)):\n",
    "        \n",
    "        cxv = X_test2_gru_multi.iloc[idx]\n",
    "        cxv = torch.tensor(cxv)\n",
    "        cxv = torch.reshape(cxv,(1,300)).float()\n",
    "        probs_t = gru_my_model_multi(cxv)\n",
    "        vcx = torch.tensor([y_test2_gru_multi.iloc[idx]])\n",
    "        probs_t = torch.reshape(probs_t,(1,3))\n",
    "        test_pred.append(torch.argmax(probs_t, dim=1).numpy()[0])\n",
    "        orig_label.append(vcx.numpy()[0])\n",
    "print(classification_report(orig_label,test_pred))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}